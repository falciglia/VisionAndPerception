{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project_dataPreprocessing_v1.ipynb","provenance":[],"collapsed_sections":["SDXEf5HJ14mB"],"machine_shape":"hm","mount_file_id":"1LC9kC9c9zSP2osKigAR5l5TVFbFHZlRx","authorship_tag":"ABX9TyMmtwtIsJfEBIOjrGVm7+ep"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1c766e1186f74d62a7e108172c6e5df0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01e6b087f59147eaa2da670b0886636d","IPY_MODEL_aeb7191cd3804a0591fd715ad6242cbb","IPY_MODEL_2e4618fc52f24b81b076f9c4fd55a0c3"],"layout":"IPY_MODEL_ef345ee95e4f4223899fe54a47eab749"}},"01e6b087f59147eaa2da670b0886636d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2598b5791ada4c7c926d29c9b6426ddc","placeholder":"​","style":"IPY_MODEL_20543a10c5964f0bb898751c196dbea1","value":"100%"}},"aeb7191cd3804a0591fd715ad6242cbb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4809ca0048e4518ba87f2872fbaba06","max":5804,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0518aacd9ef6475c9bc7e74d575b5243","value":5804}},"2e4618fc52f24b81b076f9c4fd55a0c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac14fb3da9644b58acc9febaec7f5b85","placeholder":"​","style":"IPY_MODEL_08c729563f2f4c6bbf5ed806f52e5981","value":" 5804/5804 [05:38&lt;00:00, 17.60it/s]"}},"ef345ee95e4f4223899fe54a47eab749":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2598b5791ada4c7c926d29c9b6426ddc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20543a10c5964f0bb898751c196dbea1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4809ca0048e4518ba87f2872fbaba06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0518aacd9ef6475c9bc7e74d575b5243":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac14fb3da9644b58acc9febaec7f5b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c729563f2f4c6bbf5ed806f52e5981":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# V&P project - Image Captioning\n","\n","Preprocessing of data from COCO Dataset.  \n","\n"],"metadata":{"id":"Nw0DpHKA6r3c"}},{"cell_type":"markdown","source":["## Set Up\n","\n","Mounting Drive directory in which we will install data.\n","Importing needed libraries."],"metadata":{"id":"ZolTnDW91zWF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vY63yFkQ1slJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658131242046,"user_tz":-120,"elapsed":16854,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"ee9ce66e-e6e5-47b1-e31a-9a33209f85f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["% cd drive\n","% cd MyDrive\n","% cd VisionAndPerception\n","% cd V&P_PROJECT"],"metadata":{"id":"ZQHb1fPm125N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658131242047,"user_tz":-120,"elapsed":12,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"a6144f74-d01e-45ad-872f-7d4099f917f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive\n","/content/drive/MyDrive\n","/content/drive/MyDrive/VisionAndPerception\n","/content/drive/MyDrive/VisionAndPerception/V&P_PROJECT\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import random\n","import nltk\n","nltk.download('punkt')\n","import pickle\n","from shutil import copyfile\n","from collections import Counter\n","from PIL import Image\n","\n","import gensim.downloader\n","from gensim.models import KeyedVectors\n","\n","import torch\n","from tqdm.notebook import tqdm\n","import numpy as np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FXpd7Nzc7NWm","executionInfo":{"status":"ok","timestamp":1658131256946,"user_tz":-120,"elapsed":5336,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"a5a2149c-698a-4ed9-894e-4284257e8151"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["!pip install --upgrade gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKASqSvv4dhe","executionInfo":{"status":"ok","timestamp":1658123614957,"user_tz":-120,"elapsed":4258,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"4392c5c8-7258-49e2-a2eb-16a3dab04951"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"]}]},{"cell_type":"code","source":["import gensim\n","print(gensim.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oaekG5c64kKY","executionInfo":{"status":"ok","timestamp":1658123614958,"user_tz":-120,"elapsed":7,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"ce7ba62c-56e7-4a95-a593-886194f62596"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.2.0\n"]}]},{"cell_type":"markdown","source":["## Data preprocessing\n","\n","Downloading the dataset and assembling the data are one-time processing\n","steps. If you are rerunning the model to resume or restart the training, then\n","you do not need to repeat the steps inside this point."],"metadata":{"id":"gEELZsT_SZrk"}},{"cell_type":"markdown","source":["### Download Train- Val- Test- Data"],"metadata":{"id":"SDXEf5HJ14mB"}},{"cell_type":"code","source":["# Create directory coco_data to contain our data\n","!mkdir coco_data"],"metadata":{"id":"0F4tk5Qd1225","executionInfo":{"status":"ok","timestamp":1652476482994,"user_tz":-120,"elapsed":12,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c6504f61-cb21-4235-d959-e751b0fb32ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘coco_data’: File exists\n"]}]},{"cell_type":"code","source":["# Download Train- Data\n","!wget http://images.cocodataset.org/zips/train2017.zip\n","!unzip ./train2017.zip -d ./coco_data/\n","!rm ./train2017.zip\n","\n","# Download Val- Data\n","!wget http://images.cocodataset.org/zips/val2017.zip\n","!unzip ./val2017.zip -d ./coco_data/\n","!rm ./val2017.zip\n","\n","\"\"\"# Download Test- Data\n","!wget http://images.cocodataset.org/zips/test2017.zip\n","!unzip ./test2017.zip -d ./coco_data/\n","!rm ./test2017.zip\"\"\"\n","\n","# Download Train- and Val- annotations\n","!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","!unzip ./annotations_trainval2017.zip -d ./coco_data/\n","!rm ./annotations_trainval2017.zip\n","!rm ./coco_data/annotations/person_keypoints_train2017.json\n","!rm ./coco_data/annotations/person_keypoints_val2017.json"],"metadata":{"id":"hFIfh7v7120l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (class) Assembling the data\n","\n","Since COCO Dataset is huge, in order to keep things simple and -above all- runnable on our GPU on Colab, we will consider:\n","\n","\n","*   4/91 different categories\n","*   1000 images for each category  \n","\n","**This would damage the performance of the proposed model, since the effective train-dataset would be composed by few images. Anyway, it allows to present in a clearer way the results from the training session and the improvement due to the attention mechanism wrt the baseline model proposed.**\n","\n"],"metadata":{"id":"lJp1WsA_2f8Z"}},{"cell_type":"code","source":["class limited_COCO_Dataset():\n","      def __init__(self,\n","                   train_instances_file,\n","                   train_caption_file,\n","                   val_instances_file,\n","                   val_caption_file,\n","                   category_list,\n","                   images_for_category):\n","          self.train_instances_file = train_instances_file\n","          self.train_caption_file = train_caption_file\n","          self.val_instances_file = val_instances_file        \n","          self.val_caption_file = val_caption_file          \n","          self.category_list = category_list\n","          self.images_for_category = images_for_category\n","\n","          # Create limited dataset\n","          self.create_limited_dataset(self.train_instances_file, self.train_caption_file, \"train\")\n","          self.create_limited_dataset(self.val_instances_file, self.val_caption_file, \"val\")\n","\n","          # Resize the images\n","          self.resize_images('./coco_data/images/train_images/', './coco_data/images/resized_train2017/', [256,256])\n","          self.resize_images('./coco_data/images/val_images/', './coco_data/images/resized_val2017/', [256,256])\n","\n","      def create_limited_dataset(self, object_file, caption_file, typo):\n","          filtered_images = self.select_images_by_categories(object_file)\n","          captions, filtered_image_file_names = self.select_captions(caption_file, filtered_images)\n","          self.write_caption_file(captions, filtered_image_file_names, typo)\n","\n","      def select_images_by_categories(self, object_file):\n","          category_dict = dict()\n","          for category_id in self.category_list:\n","              category_dict[category_id] = dict()\n","          all_images = dict()\n","          filtered_images = set()\n","\n","          with open(object_file) as json_file:\n","            object_detections = json.load(json_file)\n","          \n","          for annotation in object_detections['annotations']:\n","              category_id = annotation['category_id']\n","              image_id = annotation['image_id']\n","              area = annotation['area']\n","              if category_id in self.category_list:\n","                if image_id not in category_dict[category_id]:\n","                  category_dict[category_id][image_id] = []\n","              if image_id not in all_images:\n","                all_images[image_id] = dict()\n","              if category_id not in all_images[image_id]:\n","                all_images[image_id][category_id] = area\n","              else:\n","                current_area = all_images[image_id][category_id]\n","                if area > current_area:\n","                  all_images[image_id][category_id] = area\n","\n","          if self.images_for_category == -1:\n","            for category_id in category_dict:\n","                print(\"Processing category {}\".format(category_id))\n","                filtered_images.update(category_dict[category_id].keys())\n","                print(\"  Filtered total {} images of category {}\".format(len(category_dict[category_id].keys()), category_id))\n","          else:\n","            for image_id in all_images:\n","                areas = list(all_images[image_id].values())\n","                categories = list(all_images[image_id].keys())\n","                sorted_areas = sorted(areas, reverse=True)\n","                sorted_categories = []\n","                for area in sorted_areas:\n","                    sorted_categories.append(categories[areas.index(area)])\n","                all_images[image_id] = sorted_categories\n","\n","            for category_id in category_dict:\n","                print(\"Processing category {}\".format(category_id))\n","                for image_id in category_dict[category_id]:\n","                    category_dict[category_id][image_id] = all_images[image_id]\n","                prominance_index = 0\n","                prominent_image_ids = []\n","                while len(category_dict[category_id]) > 0 and len(prominent_image_ids) < self.images_for_category:\n","                      remaining_count = self.images_for_category - len(prominent_image_ids)\n","                      image_ids = []\n","                      for image_id in category_dict[category_id]:\n","                          if category_dict[category_id][image_id].index(category_id) == prominance_index:\n","                            image_ids.append(image_id)\n","                      for image_id in image_ids:\n","                          del category_dict[category_id][image_id]\n","                      if len(image_ids) <= remaining_count:\n","                        prominent_image_ids = prominent_image_ids + image_ids\n","                        if prominance_index > 4:\n","                          print(image_ids)\n","                        print(\"  Added all {} images at prominance_index {}\".format(len(image_ids), prominance_index))\n","                      else:\n","                        random.shuffle(image_ids)\n","                        prominent_image_ids = prominent_image_ids + image_ids[0:remaining_count]\n","                        print(\"  Added {} images at prominance_index {} out of {} images\".format(remaining_count, prominance_index, len(image_ids)))\n","                      prominance_index = prominance_index + 1\n","                filtered_images.update(prominent_image_ids)\n","                print(\"  Completed filtering of total {} images of category {}\".format(len(prominent_image_ids), category_id))\n","\n","            print(\"Processed all categories. Number of filtered images is {}\".format(len(filtered_images)))\n","            return filtered_images\n","\n","      def select_captions(self, caption_file, filtered_images):\n","          with open(caption_file) as json_file:\n","            captions = json.load(json_file)\n","\n","          filtered_annotations = []\n","          for annotation in captions['annotations']:\n","              if annotation['image_id'] in filtered_images:\n","                filtered_annotations.append(annotation)\n","          captions['annotations'] = filtered_annotations\n","          print(\"Number of filtered annotations is {}\".format(len(captions['annotations'])))\n","\n","          images = []\n","          filtered_image_file_names = set()\n","          for image in captions['images']:\n","              if image['id'] in filtered_images:\n","                images.append(image)\n","                filtered_image_file_names.add(image['file_name'])\n","          captions['images'] = images\n","          print(\"Expected number of filtered images is {}, actual number is {}\".format(len(filtered_images), len(captions['images'])))\n","          return captions, filtered_image_file_names\n","      \n","      def write_caption_file(self, captions, filtered_image_file_names, typo):\n","          with open(\"./coco_data/{}_captions.json\".format(typo), 'w+') as output_file:\n","            json.dump(captions, output_file)\n","\n","          for file_name in filtered_image_file_names:\n","              copyfile(\"./coco_data/images/{}2017/{}\".format(typo, file_name),\n","                       \"./coco_data/images/{}_images/{}\".format(typo, file_name))\n","              \n","      def resize_images(self, input_path, output_path, new_size):\n","          if not os.path.exists(output_path):\n","            os.makedirs(output_path)\n","          image_files = os.listdir(input_path)\n","          num_images = len(image_files)\n","          for i, img in enumerate(image_files):\n","              img_full_path = os.path.join(input_path, img)\n","              with open(img_full_path, 'r+b') as f:\n","                with Image.open(f) as image:\n","                  image = image.resize(new_size, Image.ANTIALIAS)\n","                  img_sv_full_path = os.path.join(output_path, img)\n","                  image.save(img_sv_full_path, image.format)\n","              if (i+1) % 100 == 0 or (i+1) == num_images:\n","                print(\"Resized {} out of {} total images.\".format(i+1, num_images))\n"],"metadata":{"id":"mHXQW8s-2i90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (class) Build the Vocabulary"],"metadata":{"id":"jEB3l8-RS5hI"}},{"cell_type":"code","source":["class Vocabulary(object):\n","    def __init__(self):\n","        self.token_to_int = {}\n","        self.int_to_token = {}\n","        self.current_index = 0\n","\n","    def __call__(self, token):\n","        if not token in self.token_to_int:\n","            return self.token_to_int['<unk>']\n","        return self.token_to_int[token]\n","\n","    def __len__(self):\n","        return len(self.token_to_int)\n","\n","    def add_token(self, token):\n","        if not token in self.token_to_int:\n","            self.token_to_int[token] = self.current_index\n","            self.int_to_token[self.current_index] = token\n","            self.current_index += 1"],"metadata":{"id":"JYInK9myS4vc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_vocabulary(json_path_train, json_path_val, threshold_occurences):\n","  with open(json_path_train) as json_file_train:\n","    captions_train = json.load(json_file_train)\n","  with open(json_path_val) as json_file_val:\n","    captions_val = json.load(json_file_val)\n","  counter = Counter()\n","  i = 0\n","  for annotation in captions_train['annotations']:\n","    i = i + 1\n","    caption = annotation['caption']\n","    tokens = nltk.tokenize.word_tokenize(caption.lower())\n","    counter.update(tokens)\n","    if i % 1000 == 0 or i == len(captions_train['annotations']):\n","      print(\"Tokenized {} out of total {} captions.\".format(i, len(captions_train['annotations'])))\n","  j = 0\n","  for annotation in captions_val['annotations']:\n","    j = j + 1\n","    caption = annotation['caption']\n","    tokens = nltk.tokenize.word_tokenize(caption.lower())\n","    counter.update(tokens)\n","    if j % 1000 == 0 or j == len(captions_val['annotations']):\n","      print(\"Tokenized {} out of total {} captions.\".format(j, len(captions_val['annotations'])))\n","\n","  tokens = [tkn for tkn, i in counter.items() if i >= threshold_occurences]\n","\n","  vocabulary = Vocabulary()\n","  vocabulary.add_token('<pad>')\n","  vocabulary.add_token('<start>')\n","  vocabulary.add_token('<end>')\n","  vocabulary.add_token('<unk>')\n","\n","  for i, token in enumerate(tokens):\n","    vocabulary.add_token(token)\n","  return vocabulary"],"metadata":{"id":"IztSic_cTQoH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Build Dataset, Vocabulary and Embeddings"],"metadata":{"id":"EXwbpV2XXS5J"}},{"cell_type":"code","source":["# Build dataset\n","train_instances_file = \"./coco_data/annotations/instances_train2017.json\"\n","train_caption_file = \"./coco_data/annotations/captions_train2017.json\"\n","val_instances_file = \"./coco_data/annotations/instances_val2017.json\"\n","val_caption_file = \"./coco_data/annotations/captions_val2017.json\"\n","category_list = [5, 7, 37, 77]\n","images_for_category = 1000\n","## 5   airplane\t    vehicle\n","## 7\t train\t      vehicle\n","## 37\t sportsball\t  sports\n","## 77\t cell phone\t  electronic\n","!mkdir coco_data/train_images\n","!mkdir coco_data/val_images\n","limited_COCO_Dataset(train_instances_file,\n","                     train_caption_file,\n","                     val_instances_file,\n","                     val_caption_file,\n","                     category_list,\n","                     images_for_category)\n","!rm -rf ./coco_data/train2017\n","!rm -rf ./coco_data/val2017\n","!rm -rf ./coco_data/annotations\n","!rm -rf ./coco_data/train_images\n","!rm -rf ./coco_data/val_images"],"metadata":{"id":"pYNrsy4yU6rd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Open pretrained word embeddings Gensim\n","weights = gensim.downloader.load(\"glove-wiki-gigaword-300\") # vectors_dim = 300"],"metadata":{"id":"tSUC0CDA2iz7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658123842558,"user_tz":-120,"elapsed":168077,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"26139e90-910f-4f40-943b-72560f11c5fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 376.1/376.1MB downloaded\n"]}]},{"cell_type":"code","source":["# Build vocabulary\n","vocabulary = build_vocabulary('/content/drive/MyDrive/VisionAndPerception/V&P_PROJECT/coco_data/train_captions.json',\n","                              '/content/drive/MyDrive/VisionAndPerception/V&P_PROJECT/coco_data/val_captions.json',\n","                              threshold_occurences=1)\n","print(\"Total vocabulary size: {}\".format(len(vocabulary)))"],"metadata":{"id":"f2Igz10s5D8l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658131304531,"user_tz":-120,"elapsed":3361,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"9ae54f35-9a1e-436a-c7fa-9cdc93da7b8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized 1000 out of total 20016 captions.\n","Tokenized 2000 out of total 20016 captions.\n","Tokenized 3000 out of total 20016 captions.\n","Tokenized 4000 out of total 20016 captions.\n","Tokenized 5000 out of total 20016 captions.\n","Tokenized 6000 out of total 20016 captions.\n","Tokenized 7000 out of total 20016 captions.\n","Tokenized 8000 out of total 20016 captions.\n","Tokenized 9000 out of total 20016 captions.\n","Tokenized 10000 out of total 20016 captions.\n","Tokenized 11000 out of total 20016 captions.\n","Tokenized 12000 out of total 20016 captions.\n","Tokenized 13000 out of total 20016 captions.\n","Tokenized 14000 out of total 20016 captions.\n","Tokenized 15000 out of total 20016 captions.\n","Tokenized 16000 out of total 20016 captions.\n","Tokenized 17000 out of total 20016 captions.\n","Tokenized 18000 out of total 20016 captions.\n","Tokenized 19000 out of total 20016 captions.\n","Tokenized 20000 out of total 20016 captions.\n","Tokenized 20016 out of total 20016 captions.\n","Tokenized 1000 out of total 3162 captions.\n","Tokenized 2000 out of total 3162 captions.\n","Tokenized 3000 out of total 3162 captions.\n","Tokenized 3162 out of total 3162 captions.\n","Total vocabulary size: 5804\n"]}]},{"cell_type":"code","source":["# Build embeddings\n","def adjust_weights_gensim(weights: KeyedVectors, vocab, vectors_dim):\n","    vectors = weights.vectors\n","    pretrained_embeddings = torch.randn(len(vocab), vectors_dim)\n","    initialised = 0\n","    progress_bar = tqdm(range(len(vocabulary.int_to_token)))\n","    for i, w in enumerate(vocabulary.int_to_token):\n","        if w == '<pad>':\n","          pretrained_embeddings[i] = torch.FloatTensor(np.random.rand(1, vectors.shape[1])).to(\"cuda\")\n","        if w == '<unk>':\n","          pretrained_embeddings[i] = torch.FloatTensor(np.mean(vectors, axis=0, keepdims=True)).to(\"cuda\")\n","        if w in weights.index_to_key:\n","          vec = weights[w]\n","          pretrained_embeddings[i] = torch.FloatTensor(vec).to(\"cuda\")\n","        else: # if w not in weights.index_to_key\n","          pretrained_embeddings[i] = torch.FloatTensor(np.mean(vectors, axis=0, keepdims=True)).to(\"cuda\")\n","\n","        progress_bar.update()\n","    progress_bar.close()\n","    return pretrained_embeddings"],"metadata":{"id":"NK7sjM0b5imU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectors_dim= 300\n","embeddings = adjust_weights_gensim(weights, vocabulary, vectors_dim)"],"metadata":{"id":"bj8u9M-g6DGQ","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1c766e1186f74d62a7e108172c6e5df0","01e6b087f59147eaa2da670b0886636d","aeb7191cd3804a0591fd715ad6242cbb","2e4618fc52f24b81b076f9c4fd55a0c3","ef345ee95e4f4223899fe54a47eab749","2598b5791ada4c7c926d29c9b6426ddc","20543a10c5964f0bb898751c196dbea1","d4809ca0048e4518ba87f2872fbaba06","0518aacd9ef6475c9bc7e74d575b5243","ac14fb3da9644b58acc9febaec7f5b85","08c729563f2f4c6bbf5ed806f52e5981"]},"executionInfo":{"status":"ok","timestamp":1658124503507,"user_tz":-120,"elapsed":338516,"user":{"displayName":"salvatore falciglia","userId":"13646007951713299711"}},"outputId":"a4fe837e-b257-422d-9474-4a6bb6cc28b2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5804 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c766e1186f74d62a7e108172c6e5df0"}},"metadata":{}}]},{"cell_type":"markdown","source":["### Save Vocabulary and Embeddings"],"metadata":{"id":"F6yTChcg8aQb"}},{"cell_type":"code","source":["def save_vocabulary(vocabulary):\n","\t\tdestination_file = open(\"./coco_data/vocabulary.pkl\", \"wb\")\n","\t\tpickle.dump(vocabulary, destination_file)\n","\t\tdestination_file.close()\n","  \n","def save_embeddings(embeddings):\n","\t\tdestination_file = open(\"./coco_data/embeddings.pkl\", \"wb\")\n","\t\tpickle.dump(embeddings, destination_file)\n","\t\tdestination_file.close()"],"metadata":{"id":"SJhft6FG3y6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_embeddings(embeddings)\n","save_vocabulary(vocabulary)"],"metadata":{"id":"ReywfGIK38LY"},"execution_count":null,"outputs":[]}]}